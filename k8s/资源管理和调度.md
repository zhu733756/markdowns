<!--
 * @Description:
 * @version:
 * @Author: zhu733756
 * @Date: 2020-09-08 17:06:09
 * @LastEditors: zhu733756
 * @LastEditTime: 2020-09-09 14:56:12
-->

## 资源管理

#### cpu

```yaml
spec.containers[].resources.limits.cpu
spec.containers[].resources.limits.memory
spec.containers[].resources.requests.cpu
spec.containers[].resources.requests.memory
```

- cpu=1 指的就是，这个 Pod 的 CPU 限额是 1 个 CPU。
  - 具体 1 个 CPU 在宿主机上如何解释，是 1 个 CPU 核心，还是 1 个 vCPU，还是 1 个 CPU 的超线程（Hyperthread），完全取决于宿主机的 CPU 实现方式。
- 单位
  - Ei、Pi、Ti、Gi、Mi、Ki（或者 E、P、T、G、M、K）
- 1Mi=1024*1024；1M=1000*1000
- requests.cpu=250m
  - Cgroups 的 cpu.shares 的值设置为 (250/1000)*1024
- limits.cpu=500m
  - Cgroups 的 cpu.cfs_quota_us 的值设置为 (500/1000)*100ms
- limits.memory=128Mi
  - Cgroups 的 memory.limit_in_bytes 设置为 128*1024*1024

#### QoS
- Guaranteed 【保证的】
- Burstable 【最高容许的】
- BestEffort 【尽力的】

#### Eviction阈值
```yaml
memory.available<100Mi
nodefs.available<10%
nodefs.inodesFree<5%
imagefs.available<15%
```

#### 建议
- 将 DaemonSet 的 Pod 都设置为 Guaranteed 的 QoS 类型

## 调度器

- 调度器选择最合适的node
  - 用一组叫作 Predicate 的调度算法，来检查每个 Node, 返回可用主机列表
    - GeneralPredicates
    - 与 Volume 相关的过滤规则
      - Local Persistent Volume（本地持久化卷），必须使用 nodeAffinity 来跟某个具体的节点绑定
      - 调度器还要负责检查所有待绑定 PV
    - 宿主机相关的过滤规则
      - PodToleratesNodeTaints，负责检查的就是我们前面经常用到的 Node 的“污点”机制
    - Pod 相关的过滤规则
      - PodAffinityPredicate
        - 检查待调度 Pod 与 Node 上的已有 Pod 之间的亲密（affinity）和反亲密（anti-affinity）关系
  - 再调用一组叫作 Priority 的调度算法，来给上一步得到的结果里的每个 Node 打分
    - 这个算法实际上就是在选择空闲资源（CPU 和 Memory）最多的宿主机

- 调度机制的工作原理
  - informer path 【生成任务队列】
    - FIFO/priority path
    - scheduler cache
      - 同步bind
  - scheduling path
    - 优先更新pod和node【Assume】
    - 之后再异步更新pod、异步bind
  - 运行成功后
    - Admit 

## 优先级（Priority ）和抢占（Preemption）机制
```
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for high priority service pods only."
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority
```
- 抢占
  - 当一个高优先级的 Pod 调度失败的时候，调度器的抢占能力就会被触发。

- 当整个集群发生可能会影响调度结果的变化（比如，添加或者更新 Node，添加和更新 PV、Service 等）时，调度器会执行一个被称为 MoveAllToActiveQueue 的操作，把所调度失败的 Pod 从 unscheduelableQ 移动到 activeQ 里面。请问这是为什么？

- 一个相似的问题是，当一个已经调度成功的 Pod 被更新时，调度器则会将 unschedulableQ 里所有跟这个 Pod 有 Affinity/Anti-affinity 关系的 Pod，移动到 activeQ 里面。请问这又是为什么呢？

- 因为第一种情况下集群资源发生了变化

- 第二种情况是放pod调度成功后，跟这个pod有亲和性和反亲和性规则的pod需要重新过滤一次可用节点。